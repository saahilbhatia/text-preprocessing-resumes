{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "Libraries used in the task:\n",
    "* re (for regular expression) \n",
    "* nltk (for text pre-processing)\n",
    "* codecs(for reading files)\n",
    "* os(for IO operations)\n",
    "* itertools (using the chain iterator to simplify nested loops/iterations)\n",
    "* numpy (for calculating statistics)  \n",
    "\n",
    "## 1. Introduction\n",
    "This notebook preprocesses a set of resume files and generates a sparse representation of the resumes. The dataset contains 250 resumes, which includes information of the applicants like contact details, skills, work experience, educational background, hobbies, etc.\n",
    "\n",
    "The resumes containting the information of the applicants were loaded and any duplicate resumes were excluded from the analysis. Text pre-processing was performed on the resume files in the dataset. The pre-processing included case normalization, tokenization, removal of stopwords and stemming. Additionally, the most frequent and rare tokens were removed, the tokens with length less than 3 were removed, and the first 200 meaningful bigrams were added to the vocabulary. The capital tokens appearing in the middle of a sentence/line were not normalized to lower case because of the hypothesis that these tokens are likely to have a different meaning than their lower case counterparts. \n",
    "\n",
    "The notebook concludes with the generation of the lexical vocabulary and the count vector sparse representations of the resumes. The vocabulary and vector representations are written to files, which can be used as input to various recommender-systems and information retrieval algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to import libraries that are needed for this assessment:\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import os\n",
    "import codecs \n",
    "import itertools\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Loading the resumes\n",
    "\n",
    "In this section, the resumes needed for this task are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resume IDs\n",
    "resume_ids = [     685,467,555,22,534,541,359,787,3,265,606,73,144,178,\n",
    "                   151,259,840,35,433,260,266,408,697,391,335,823,293,733,\n",
    "                   194,1,643,384,372,93,226,709,760,857,291,817,488,482,\n",
    "                   781,706,117,323,260,403,234,848,285,727,55,861,370,590,\n",
    "                   704,510,819,136,577,451,619,291,224,825,259,258,477,575,\n",
    "                   432,715,12,699,855,13,204,486,361,665,583,686,163,764,\n",
    "                   391,61,507,147,127,316,537,113,361,857,166,328,577,205,\n",
    "                   792,198,805,769,294,837,328,715,678,726,287,855,178,449,\n",
    "                   512,315,453,710,546,619,487,593,329,748,376,92,437,816,\n",
    "                   324,372,183,231,482,324,132,114,812,637,803,283,256,410,\n",
    "                   806,724,863,776,314,326,833,277,474,664,857,126,365,578,\n",
    "                   10,354,70,32,472,536,150,670,230,765,562,52,551,841,\n",
    "                   85,73,322,827,576,592,211,365,205,353,546,326,64,627,\n",
    "                   242,850,743,507,183,365,475,240,515,694,77,838,217,649,\n",
    "                   431,315,260,686,139,823,617,194,690,481,245,87,131,665,\n",
    "                   546,594,6,507,96,54,449,856,530,805,463,484,682,320,364,\n",
    "                   688,666,413,22,340,577,612,348,453,538,245,641,12,483,\n",
    "                   505,642,470,107,17,328,226,179,682,252,61\n",
    "             ]\n",
    "len(resume_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with any loading/pre-processing tasks, I am checking for **duplicate resume IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ids = set(resume_ids)\n",
    "len(unique_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 250 resume IDs, but only 207 of them are unique. I am using the **207 unique resumes** to complete the task. <br>\n",
    "\n",
    "Loading the resumes and storing them as **dictionary** with the key as the ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IDs range from 1 to 863 but they aren't continuous.\n"
     ]
    }
   ],
   "source": [
    "# Sorting IDs before iterating through them\n",
    "sorted_ids = list(unique_ids)  \n",
    "sorted_ids.sort()\n",
    "print('The IDs range from',min(sorted_ids), 'to', max(sorted_ids),\"but they aren't continuous.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading** all the resume files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "resumes_dict = {}\n",
    "for i in range(1,864):\n",
    "    if i in sorted_ids:\n",
    "        file_contents = []\n",
    "        name = 'resume_(' + str(i) + ').txt'\n",
    "        # Note that the files are saved in A1-Task2-Resumes folder for me,\n",
    "        # you may need to change the path based on the location of your file.\n",
    "        filepath = os.path.join(cwd,'Resumes',name)  \n",
    "        file = codecs.open(filepath,mode='r',encoding='utf-8')\n",
    "        file_contents = file.read()\n",
    "        resumes_dict[i] = file_contents\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\r\\n\\r\\n                       Contact: (+65) 8123-1544 (Mobile) \\r\\n \\r\\n           Nationality: Singaporean \\r\\n \\r\\n\\r\\nTeo Kai Zhi Terence, CAIA \\r\\nAddress: Blk 366 Bukit Batok Street 31, #04-269 \\r\\nEmail: terencettkz@gmail.com        \\r\\n \\r\\n \\r\\n \\r\\n          \\r\\nWorking Experience \\r\\nCitco Fund Services, Singapore \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\nRole: Senior Fund Accountant, Private Equity & Real Estate Team \\r\\n!  Preparing monthly and quarterly reports for Private Equity Funds \\r\\n!  Performing weekly and monthly custodian and NAV reconciliations \\r\\n!  Private Equity accounting using Investran (Distributions, Capital Calls and Partner’s Account Summary) \\r\\n!  Maintaining day-to-day relationship with investment managers, brokers and auditors \\r\\n!  Working with Traditional and Alternative investments \\r\\n!  Price checking Hedge Fund’s portfolio using various providers \\r\\n\\r\\n \\r\\n\\r\\n                  2014 – Present \\r\\n\\r\\n \\r\\nEducation \\r\\nUniversity of London (UOL), Singapore Institute of Management \\r\\nBSc (Hons) Banking and Finance \\r\\n  \\r\\n             \\r\\n \\r\\n!  Achieved Second Class Honours (Upper) \\r\\n!  Distinctions in: \\r\\n\"  Investment Management \\r\\n\"  Valuation Securities Analysis \\r\\n\"  Financial Reporting \\r\\n!  School activities: \\r\\n\"  Investment and Networking Club Committee Member – Research Executive  \\r\\n \\r\\n\\r\\n                 2011 – 2014 \\r\\n\\r\\nSkills and Accomplishments \\r\\n!  Chartered Alternative Investment Analyst (CAIA) Certified \\r\\n!  Pursuing Chartered Financial Analyst (CFA) Level 1 Exam \\r\\n! \\r\\nInvestran Private Equity Enterprise Application \\r\\n!  Bloomberg Terminal Experience \\r\\n!  Certified Microsoft Office Specialist: Excel 2010 Expert \\r\\n!  Bloomberg Aptitude Test (BAT) – Achieved Top 5 in Asia Pacific for Dec 2013 \\r\\n!  2014 SIM Re Suisse Capital FX Competition – First Place Winner \\r\\n\\r\\n\\x0c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes_dict[151] # Checking that the files are correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that no resume is empty\n",
    "empty_resumes = [k for k, v in resumes_dict.items() if len(v) == 0]\n",
    "empty_resumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.1 Case Normalization  \n",
    "\n",
    " \n",
    "Doing **case normalization** for words based on their occurrence in the middle of a line/sentence. The words in the middle of a sentence are not normalized, remaining words are normalized. The hypothesis here is that the capitalized words in the middle of a sentence have a different meaning compared to their lowercase counterparts. For example, 'Word' in the middle of a sentence could be referring to MS Word; so, 'Word' has a different meaning than 'word' when it appears in the middle of a sentence. \n",
    " \n",
    " The first step would be to split the resumes to sentences. Peforming **sentence segmentation** using the **nltk punkt sentence tokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Segmentation\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences_resumes = {}\n",
    "for key, resume in resumes_dict.items():\n",
    "    sentences = sent_detector.tokenize(resume.strip())\n",
    "    sentences_resumes[key] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contact: (+65) 8123-1544 (Mobile) \\r\\n \\r\\n           Nationality: Singaporean \\r\\n \\r\\n\\r\\nTeo Kai Zhi Terence, CAIA \\r\\nAddress: Blk 366 Bukit Batok Street 31, #04-269 \\r\\nEmail: terencettkz@gmail.com        \\r\\n \\r\\n \\r\\n \\r\\n          \\r\\nWorking Experience \\r\\nCitco Fund Services, Singapore \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\nRole: Senior Fund Accountant, Private Equity & Real Estate Team \\r\\n!',\n",
       " 'Preparing monthly and quarterly reports for Private Equity Funds \\r\\n!',\n",
       " 'Performing weekly and monthly custodian and NAV reconciliations \\r\\n!',\n",
       " 'Private Equity accounting using Investran (Distributions, Capital Calls and Partner’s Account Summary) \\r\\n!',\n",
       " 'Maintaining day-to-day relationship with investment managers, brokers and auditors \\r\\n!',\n",
       " 'Working with Traditional and Alternative investments \\r\\n!',\n",
       " 'Price checking Hedge Fund’s portfolio using various providers \\r\\n\\r\\n \\r\\n\\r\\n                  2014 – Present \\r\\n\\r\\n \\r\\nEducation \\r\\nUniversity of London (UOL), Singapore Institute of Management \\r\\nBSc (Hons) Banking and Finance \\r\\n  \\r\\n             \\r\\n \\r\\n!',\n",
       " 'Achieved Second Class Honours (Upper) \\r\\n!',\n",
       " 'Distinctions in: \\r\\n\"  Investment Management \\r\\n\"  Valuation Securities Analysis \\r\\n\"  Financial Reporting \\r\\n!',\n",
       " 'School activities: \\r\\n\"  Investment and Networking Club Committee Member – Research Executive  \\r\\n \\r\\n\\r\\n                 2011 – 2014 \\r\\n\\r\\nSkills and Accomplishments \\r\\n!',\n",
       " 'Chartered Alternative Investment Analyst (CAIA) Certified \\r\\n!',\n",
       " 'Pursuing Chartered Financial Analyst (CFA) Level 1 Exam \\r\\n!',\n",
       " 'Investran Private Equity Enterprise Application \\r\\n!',\n",
       " 'Bloomberg Terminal Experience \\r\\n!',\n",
       " 'Certified Microsoft Office Specialist: Excel 2010 Expert \\r\\n!',\n",
       " 'Bloomberg Aptitude Test (BAT) – Achieved Top 5 in Asia Pacific for Dec 2013 \\r\\n!',\n",
       " '2014 SIM Re Suisse Capital FX Competition – First Place Winner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_resumes[151] # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Case normalizing** the first word of every sentence to lower case for the result obtained from the sentence tokenizer.  \n",
    " \n",
    " Defining **functions** to do the case normalization for every sentence. The steps to achieve this are:\n",
    " - Capture the first word of the sentence, even if it is present after a bullet\n",
    " - If the first word captured is completely in upper case, leave it as is. For example, CEO is left as is\n",
    " - Convert any first word, apart from all upper case first words, to lower case. For example, Work converted to work \n",
    " \n",
    " A regular expression is written to capture the first word of a sentence; the regex looks for the first word in a new line, bullets followed by a space character before the first word are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_repl(matchobj):\n",
    "    if matchobj.group(0).isupper(): # If all CAPS, then no normalization. For Example: CGMA, CEO, etc.\n",
    "        return matchobj.group(0)\n",
    "    else:\n",
    "        return matchobj.group(0).lower() # Else, normalize to lower case\n",
    "\n",
    "def casenormalize(sentence):\n",
    "    # regex to capture the first word of a sentence\n",
    "    sentence = re.sub(r'^(?:(?:•|-|\\uf0b7|■])?\\s*)?\\b(\\w+)\\b',lower_repl, sentence)\n",
    "    return sentence\n",
    "\n",
    "normalized_sent_resume = {}\n",
    "for key, resume in sentences_resumes.items():\n",
    "    resume_updated = []\n",
    "    for sentence in resume:\n",
    "        sent_updated = casenormalize(sentence)\n",
    "        resume_updated.append(sent_updated)\n",
    "    normalized_sent_resume[key] = resume_updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contact: (+65) 8123-1544 (Mobile) \\r\\n \\r\\n           Nationality: Singaporean \\r\\n \\r\\n\\r\\nTeo Kai Zhi Terence, CAIA \\r\\nAddress: Blk 366 Bukit Batok Street 31, #04-269 \\r\\nEmail: terencettkz@gmail.com        \\r\\n \\r\\n \\r\\n \\r\\n          \\r\\nWorking Experience \\r\\nCitco Fund Services, Singapore \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\nRole: Senior Fund Accountant, Private Equity & Real Estate Team \\r\\n!',\n",
       " 'preparing monthly and quarterly reports for Private Equity Funds \\r\\n!',\n",
       " 'performing weekly and monthly custodian and NAV reconciliations \\r\\n!',\n",
       " 'private Equity accounting using Investran (Distributions, Capital Calls and Partner’s Account Summary) \\r\\n!',\n",
       " 'maintaining day-to-day relationship with investment managers, brokers and auditors \\r\\n!',\n",
       " 'working with Traditional and Alternative investments \\r\\n!',\n",
       " 'price checking Hedge Fund’s portfolio using various providers \\r\\n\\r\\n \\r\\n\\r\\n                  2014 – Present \\r\\n\\r\\n \\r\\nEducation \\r\\nUniversity of London (UOL), Singapore Institute of Management \\r\\nBSc (Hons) Banking and Finance \\r\\n  \\r\\n             \\r\\n \\r\\n!',\n",
       " 'achieved Second Class Honours (Upper) \\r\\n!',\n",
       " 'distinctions in: \\r\\n\"  Investment Management \\r\\n\"  Valuation Securities Analysis \\r\\n\"  Financial Reporting \\r\\n!',\n",
       " 'school activities: \\r\\n\"  Investment and Networking Club Committee Member – Research Executive  \\r\\n \\r\\n\\r\\n                 2011 – 2014 \\r\\n\\r\\nSkills and Accomplishments \\r\\n!',\n",
       " 'chartered Alternative Investment Analyst (CAIA) Certified \\r\\n!',\n",
       " 'pursuing Chartered Financial Analyst (CFA) Level 1 Exam \\r\\n!',\n",
       " 'investran Private Equity Enterprise Application \\r\\n!',\n",
       " 'bloomberg Terminal Experience \\r\\n!',\n",
       " 'certified Microsoft Office Specialist: Excel 2010 Expert \\r\\n!',\n",
       " 'bloomberg Aptitude Test (BAT) – Achieved Top 5 in Asia Pacific for Dec 2013 \\r\\n!',\n",
       " '2014 SIM Re Suisse Capital FX Competition – First Place Winner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sent_resume[151] # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case normalized **sentences** of a resume are **joint** to get the **case normalized resumes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_resumes = {}\n",
    "for key, sent_list in normalized_sent_resume.items():\n",
    "    resume = ''.join(sent_list)\n",
    "    normalized_resumes[key] = resume"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tokenization\n",
    "\n",
    "Performing the **word tokenization** using regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_dict = {}\n",
    "tokenizer = RegexpTokenizer(\"\\w+(?:[-']\\w+)?\", gaps=False)\n",
    "for key, resume in normalized_resumes.items():\n",
    "    tokens = tokenizer.tokenize(resume)\n",
    "    tokens_dict[key] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vitae'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict[1][1] # Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generating 200 meaningful bigrams \n",
    "A bigram is a pair of consecutive tokens. The nltk collocations packages is used to identify bigrams in the dataset.  \n",
    "\n",
    "Before we generate the bigrams, we need to concatenate all the tokenized resumes; this was done with the help of chain_from_iterable function.The list returned by the function contains all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens is 134220\n",
      "The number of words in the vocabulary is 16443\n"
     ]
    }
   ],
   "source": [
    "all_words = list(chain.from_iterable(tokens_dict.values()))\n",
    "voc = list(set(all_words))\n",
    "print('The number of tokens is',len(all_words))\n",
    "print('The number of words in the vocabulary is',len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating **first 200 bigrams** using functions from the nltk.collocations package.  \n",
    "The important things to note  here are:\n",
    "- Frequency filter is set at 10; the bigrams must have a **frequency > 10**\n",
    "- Tokens with length < 3, stopwords, and tokens with only numbers are **ignored** in the analysis\n",
    "- **Pointwise mutual information (PMI)** measure is used to identify the best 200 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = []\n",
    "with open('stopwords_en.txt','r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip('\\n')\n",
    "        stopwords_list.append(word)\n",
    "file.close()\n",
    "stopwords = set(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Merrill', 'Lynch'),\n",
       " ('Ngee', 'Ann'),\n",
       " ('Sdn', 'Bhd'),\n",
       " ('Abdul', 'Rahman'),\n",
       " ('Kuala', 'Lumpur')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_finder.apply_freq_filter(10)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stopwords or w.isdigit())\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "top_200_bigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the concordance method to **validate the generated bigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 14 of 14 matches:\n",
      "io ns Lia ise with internat io na l c lie nts a nd o ther globa l o ffic es to \n",
      "o ther globa l o ffic es to e nsure c lie nts needs a re me t Per form va lua t\n",
      " ime ly ma nner in accorda nce with c lie nts req uireme nts Inve st igated a n\n",
      "e ne va a nd Bloomberg Lia ise with c lie nts o ther C itco o ffices a nd teams\n",
      "uppor ted the o n-board ing o f new c lie nts a nd ass isted in the tra ns fer \n",
      "d in the tra ns fer o f e xis t ing c lie nts from other globa l o ffices a nd \n",
      "ra t io n fo r corpo rate ba nk ing c lie nts Ro le invo lves da ta mining for \n",
      "nvo lves da ta mining for targe ted c lie nts matc hing c lie nts p ro files p \n",
      "g for targe ted c lie nts matc hing c lie nts p ro files p la nning proce ss fl\n",
      " nd a ct ive ly fo llow ing up with c lie nts for ac t iva t io n Ge nera ted d\n",
      "tus Prob lem so lving for corporate c lie nts thro ugh te lecommun icat io ns D\n",
      " fere nt pa yme nt temp la tes fo r c lie nts in the new system JobsDB M arke t\n",
      "t ly appro ac hed new po te nt ia l c lie nts a nd ma rketed the comp a ny s pr\n",
      "a ny s prod uct to them to grow the c lie nt b ase o f the compa ny Mana ged e \n"
     ]
    }
   ],
   "source": [
    "nltk.Text(all_words).concordance('lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 15 of 15 matches:\n",
      "mberg English Mandarin and Cantonese ABDUL MUHSIN BIN ZULKIFLI Block 140 Pasir \n",
      "ion Working Experience College Tunku Abdul Rahman SMJK Sam Tet Advanced Diploma\n",
      "e College Tunku Financial Accounting Abdul Rahman CGPA 2 66 of 4 00 Diploma In \n",
      "oma in Computerized Accounting Tunku Abdul Rahman College Kuala Lumpur Diploma \n",
      "EDUCATION 2009 2011 University Tunku Abdul Rahman UTAR Malaysia course Bachelor\n",
      " in Malaysia 2007 2009 College Tunku Abdul Rahman KTAR Malaysia diploma in Busi\n",
      "Pappu a p P Anthony University Tunku Abdul Rahman Final Year Project Supervisor\n",
      "ed Diploma in Business Studies Tunku Abdul Rahman College Malaysia 1999-2003 ME\n",
      " Commerce Financial Accounting Tunku Abdul Rahman College 2003 RELATED SKILLS P\n",
      "e Business Management cum ICSA Tunku Abdul Rahman College Malaysia Graduated 20\n",
      "istration Institute University Tunku Abdul Rahman College Malaysia Graduated 20\n",
      "IVITIES Assistant Group Leader Tunku Abdul Rahman College TARC Orientation Comm\n",
      " Committee 2010 2011 Represent Tunku Abdul Rahman College TARC participated in \n",
      "ntation Contest 2011 represent Tunku Abdul Rahman College TARC participated in \n",
      "ficate of Achievement F1 F2 F3 Tunku Abdul Rahman College Cambridge A Level St \n"
     ]
    }
   ],
   "source": [
    "nltk.Text(all_words).concordance('Abdul')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the results from the concordance method, we can **remove** the following bigrams:\n",
    "- ('lie' , 'nts') as it does not make sense. It is actually used to refer to clients, but there are white spaces in the word \"clients\" in the resume\n",
    "- ('Touche' , 'LLP') and ('Young' , 'LLP') because they are used to refer to Deloitte Touche LLP and Ernst Young LLP; ('Deloitte' , 'Touche') and ('Ernst' , 'Young') are already present as bigrams. Morever, 'LLP' refers to limited liability partnership; it is more meaningful to capture the organizations in a LLP rather than one organization with the token 'LLP'  \n",
    "\n",
    "\n",
    "The following bigrams need to be **modified**:  \n",
    "- Tunku Abdul Rahman College: The bigrams ('Tunku' , 'Abdul') and ('Abdul' , 'Rahman') are redundant because they refer to \"Abdul Rahman College\"; instead we can have one token - ('Abdul' , 'Rahman' , 'College') (trigram)\n",
    "- Ngee Ann Poytechnic: Similarly, the bigrams ('Ngee' , 'Ann') and ('Ann' , 'Polytechnic') are redundant because they refer to \"Ngee Ann Poytechnic\"; instead we can have one token - ('Ngee' , 'Ann', 'Polytechnic') (trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_to_delete = {\n",
    "    ('Tunku', 'Abdul'),\n",
    "    ('Abdul','Rahman'),\n",
    "    ('Ngee', 'Ann'),\n",
    "    ('Ann', 'Polytechnic'),\n",
    "    ('Touche', 'LLP'),\n",
    "    ('Young' ,'LLP'),\n",
    "    ('lie', 'nts')}\n",
    "len(bigrams_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to generate new bigrams to **replace** the bigrams we are deleting. Generating **top 210 bigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Audit', 'Manager'),\n",
       " ('Kong', 'Institute'),\n",
       " ('Senior', 'Fund'),\n",
       " ('Senior', 'Manager'),\n",
       " ('Singapore', 'Pte'),\n",
       " ('financial', 'models'),\n",
       " ('fund', 'manager'),\n",
       " ('listed', 'company'),\n",
       " ('market', 'research'),\n",
       " ('regulatory', 'reporting')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_200_bigrams = set(top_200_bigrams)\n",
    "top_210_bigrams = set(bigram_finder.nbest(bigram_measures.pmi, 210)) # Top-210 bigrams\n",
    "top_210_bigrams - top_200_bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the 5 most meaningful bigrams from the additional 10 bigrams generated based on a subjective call, and combining these 5 bigrams with the trigrams ('Tunku', 'Abdul','Rahman') and ('Ngee', 'Ann', 'Polytechnic'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_to_add = {\n",
    "    ('Audit', 'Manager'),\n",
    "    ('Senior', 'Manager'),\n",
    "    ('financial', 'models'),\n",
    "    ('fund', 'manager'),\n",
    "    ('market', 'research'),\n",
    "    ('Tunku', 'Abdul','Rahman'),\n",
    "    ('Ngee', 'Ann', 'Polytechnic')}\n",
    "len(bigrams_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_updated = top_200_bigrams - bigrams_to_delete\n",
    "bigrams_updated = bigrams_updated.union(bigrams_to_add)\n",
    "len(bigrams_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-tokenizing** the resumes to **add collocations**. We also need to ensure that the collocations are not split into two individual words. The **MWEtokenizer** takes care of that and was used to re-tokenize the resumes with collocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130369\n"
     ]
    }
   ],
   "source": [
    "mwetokenizer = MWETokenizer(bigrams_updated)\n",
    "colloc_resumes =  dict((key, mwetokenizer.tokenize(resume)) for key,resume in tokens_dict.items())\n",
    "all_words_colloc = list(chain.from_iterable(colloc_resumes.values()))\n",
    "print(len(all_words_colloc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16630\n"
     ]
    }
   ],
   "source": [
    "colloc_voc = list(set(all_words_colloc))\n",
    "print(len(colloc_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a set of the collocations, which can be used in the further steps while preprocessing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_set = set()\n",
    "for item in bigrams_updated:\n",
    "    if len(item) == 2:\n",
    "        bigrams_set.add(item[0] + '_' + item[1])\n",
    "    if len(item) == 3:\n",
    "        bigrams_set.add(item[0] + '_' + item[1] + '_' + item[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Removing very small tokens\n",
    "Very small tokens of lengths 1 and 2 characters possess very less lexical context. For example, the tokens 'V', 'B', etc. could be the initals of the applicant and do not add meaningful insights to any analysis on this dataset.\n",
    "\n",
    "**Removing very small tokens** of length less than 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeable_tokens_dict = {}\n",
    "for key, token_list in colloc_resumes.items():\n",
    "    sizeable_tok_list = []\n",
    "    for token in token_list:\n",
    "        if len(token) > 2:\n",
    "            sizeable_tok_list.append(token)\n",
    "    sizeable_tokens_dict[key] = sizeable_tok_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Removing context-independent stopwords\n",
    "Stopwords are words that are frequently used and possess very less lexical content. It is more efficient to remove stopwords for faster processing and lesser storage space.  \n",
    "\n",
    "Removing **stopwords** using the given stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_tok_dict = {}\n",
    "for key, token_list in sizeable_tokens_dict.items():\n",
    "    stopped_tokens = []\n",
    "    for token in token_list:\n",
    "        if token not in stopwords:\n",
    "            stopped_tokens.append(token)\n",
    "    stopped_tok_dict[key] = stopped_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Stemming\n",
    "\n",
    "Stemming is done to reduce derivational forms of a word to their base form (stem). For example, the words 'bake', 'baked', and  'baking' are reduced to the base form 'bake'. Hence, the stemming process includes the identification and removal of prefixes, suffixes, and pluralisation of words to their base form (stem). \n",
    "\n",
    "The capital tokens and collocations are not stemmed, only the lower case tokens are stemmed; this is because the capital tokens and collocations carry a different meaning than their stemmed lower case counterpart\n",
    "\n",
    "The tokens were stemmed using the **Porter Stemmer** from the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tok_dict = {}\n",
    "for key, token_list in stopped_tok_dict.items():\n",
    "    stemmed_tokens = []\n",
    "    for token in token_list:\n",
    "        if token not in bigrams_set and token.islower():\n",
    "            stem_token = stemmer.stem(token)\n",
    "            stemmed_tokens.append(stem_token)\n",
    "        else:\n",
    "            stemmed_tokens.append(token)\n",
    "    stemmed_tok_dict[key] = stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curriculum', 'Vitae', 'Gowribalan', 'MCSI', 'FCMA']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tok_dict[1][:5] # checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Removing context-dependent stopwords and rare tokens\n",
    "\n",
    "The goal of this task is to remove the words appearing in more than 98% documents and the words appearing in less than 2% documents.  \n",
    "\n",
    "The words appearing in more than 98% documents are referred to as context-dependent stopwords. For example, for our dataset words like work, experience, etc. could potentially appear in every resume. These words are generally removed because they aren't very useful in downstream analysis and feature extraction.\n",
    "\n",
    "The words appearing in less than 2% documents are rare tokens. For example, for our dataset names of the applicants like Gowribalan, Dwayne, Hardy, etc. appear most probably only in their individual resumes. These words are also removed because they aren't very useful in downstream analysis and feature extraction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the vocab** for the resumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  13365 \n",
      "Total number of tokens:  91713 \n",
      "Lexical diversity:  6.862177328843996\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(stemmed_tok_dict.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document frequency of a token refers to the number of documents contanining the word. For example, if the word 'manager' appears in 100 resumes, the document frequency of the word 'manager' is 100.\n",
    "\n",
    "Calculating the **document frequencies** for the words in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2 = list(chain.from_iterable([set(value) for value in stemmed_tok_dict.values()]))\n",
    "fdist = FreqDist(words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manag', 169),\n",
       " ('client', 162),\n",
       " ('University', 155),\n",
       " ('report', 152),\n",
       " ('compani', 150),\n",
       " ('team', 147),\n",
       " ('includ', 143),\n",
       " ('Bachelor', 141),\n",
       " ('work', 141),\n",
       " ('Singapore', 138),\n",
       " ('financi', 133),\n",
       " ('account', 130),\n",
       " ('2011', 130),\n",
       " ('perform', 129),\n",
       " ('gmail', 128),\n",
       " ('2012', 128),\n",
       " ('2013', 126),\n",
       " ('busi', 124),\n",
       " ('fund', 123),\n",
       " ('EDUCATION', 123),\n",
       " ('Management', 121),\n",
       " ('2015', 121),\n",
       " ('invest', 120),\n",
       " ('2014', 120),\n",
       " ('Finance', 117)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the most common words\n",
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the **threshold** for context-dependent stopwords and rare tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing most Common Words\n",
    "cutoff_most_common = round(0.98 * len(unique_ids),0)\n",
    "cutoff_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff_rare = round(0.02 * len(unique_ids),0)\n",
    "cutoff_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of points to note from the thresholds calculated above:\n",
    "- 'manag' is the most common word, appearing in 169 out of 207 resumes. However, 169 is lower than the threshold of 203 resumes for removing context-dependent stopwords. So, there are **no context-dependent stopwords** in our dataset\n",
    "- The threshold for removing **rare tokens** is **4 Resumes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_words = set([k for k, v in fdist.items() if v > cutoff_most_common])\n",
    "most_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10462"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_freq_words = set([k for k, v in fdist.items() if v < cutoff_rare])\n",
    "len(less_freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that **10,462 words out of 13,365 words appear in less than 4 resumes**. We are removing more than 75% of the tokens with the step of removing rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10462"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_delete_words = less_freq_words.union(most_freq_words)\n",
    "len(to_delete_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating clean resumes by **removing the rare tokens**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(resumeid):\n",
    "    return (resumeid, [w for w in stemmed_tok_dict[resumeid] if w not in to_delete_words])\n",
    "cleaned_resumes = dict(remove_words(resumeid) for resumeid in unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_resumes[1]) # check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Generating Final Vocabulary\n",
    "\n",
    "After all the various preprocessing steps, it is time to generate the final vocabulary for the resume dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(cleaned_resumes.values()))\n",
    "vocab = set()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing **statistics** for the final vocabulary and the cleaned documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  2903\n",
      "Total number of tokens:  75463\n",
      "Lexical diversity:  25.994832931450222\n",
      "Total number of resumes: 207\n",
      "Average document length: 364.55555555555554\n",
      "Maximum document length: 1325\n",
      "Minimum document length: 68\n",
      "Standard deviation of document length: 193.25282070562827\n"
     ]
    }
   ],
   "source": [
    "lexical_diversity = len(words) / len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab))\n",
    "print (\"Total number of tokens: \", len(words))\n",
    "print (\"Lexical diversity: \", lexical_diversity)\n",
    "print (\"Total number of resumes:\", len(unique_ids))\n",
    "lens = [len(value) for value in cleaned_resumes.values()]\n",
    "print (\"Average document length:\", np.mean(lens))\n",
    "print (\"Maximum document length:\", np.max(lens))\n",
    "print (\"Minimum document length:\", np.min(lens))\n",
    "print (\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing** the final vocabulary to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"./resumes_vocab.txt\", 'w')\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for word in vocab:\n",
    "    vocab_dict[word] = i\n",
    "    i += 1\n",
    "    \n",
    "for index, word in vocab_dict.items():\n",
    "        out_file.write(\"{}:{} \".format(index,word))\n",
    "        out_file.write('\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating sparse count vectors\n",
    "\n",
    "The sparse count vectors are the **numerical representations** of the document. The count vector is a vector with the values of the **token index** and the **token's frequency** in the document.   \n",
    "\n",
    "Note that the spare count vector of a document does not contain the indices of all words in the vocab, it only contains the indices of the words present in the document. This helps us to save space and computational difficulty.  \n",
    "\n",
    "The sparse count vector is generated using the FreqDist class from the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_2 = open(\"./resumes_countVec.txt\", 'w')\n",
    "\n",
    "for key, resume in cleaned_resumes.items():\n",
    "    out_file_2.write(str(key) + ',')\n",
    "    token_idx = [vocab_dict[w] for w in resume]\n",
    "    for k, v in FreqDist(token_idx).items():\n",
    "        out_file_2.write(\"{}:{},\".format(k,v))\n",
    "    out_file_2.write('\\n')\n",
    "out_file_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "The task demonstrated the steps for preprocessing a text file and converting it into a numerical representation while preserving the key features of the text. The output of the task can be a suitable input for downstream analysis, information retrieval algorithms and recommender systems. The statistics below depict that through basic preprocessing steps, we were able to reduce the size of the vocabulary from 16k words to 3k words and significantly improve the lexical diversity by more than 300%.  \n",
    "\n",
    "**Text Statistics Before Preprocessing:**  \n",
    "\n",
    "Total number of tokens: 130,369    \n",
    "Vocabulary size: 16,630  \n",
    "Lexical diversity is :  7.84 %  \n",
    "\n",
    "**Text Statistics After Preprocessing:**    \n",
    "\n",
    "Total number of tokens: 75,463<br>\n",
    "Vocabulary size: 2,903<br>\n",
    "Lexical diversity is : 25.99 %<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "\n",
    "- Jay. (2008, September 9). *Character reading from file in Python* [Response to]. Retrieved from https://stackoverflow.com/questions/147741/character-reading-from-file-in-python\n",
    "- *Bigram*. Retrieved from https://en.wikipedia.org/wiki/Bigram\n",
    "- *Stopwords*. Retrieved from https://en.wikipedia.org/wiki/Stop_words\n",
    "- NLTK Project. (2017). *NLTK 3.0 documentation: `nltk.tokenize.regexp` module*. Retrieved from http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer\n",
    "- NLTK Project. (2015). *Collocations*. Retrieved from http://www.nltk.org/howto/collocations.html\n",
    "- The Python Standard Library. *Regular expression operations documentation: `re.sub`*. Retrieved from https://docs.python.org/3/library/re.html   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
